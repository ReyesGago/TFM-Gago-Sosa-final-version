{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjsuKzEJTL1x"
      },
      "source": [
        "## I copy the script information to count the terms from the Wikipedia file and extract all the lists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwo1T-SXWoyB"
      },
      "source": [
        "## Code I used to initially extract the terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-0rwyo-7TCym"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# Function to count the occurrences of a term in a text file\n",
        "def count_term_occurrences(term, text_file):\n",
        "    \"\"\"\n",
        "    Counts the number of occurrences of a term in a text file, case-insensitively.\n",
        "\n",
        "    Args:\n",
        "        term (str): The term to search for.\n",
        "        text_file (str): Path to the text file.\n",
        "\n",
        "    Returns:\n",
        "        int: The number of occurrences of the term in the text file.\n",
        "    \"\"\"\n",
        "    with open(text_file, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "        return content.lower().count(term.lower())  # Case-insensitive count\n",
        "\n",
        "# Function to extract unique terms from the first column of a CSV file\n",
        "def extract_unique_terms(csv_file):\n",
        "    \"\"\"\n",
        "    Extracts unique terms from the first column of a CSV file.\n",
        "\n",
        "    Args:\n",
        "        csv_file (str): Path to the CSV file.\n",
        "\n",
        "    Returns:\n",
        "        set: A set of unique terms.\n",
        "    \"\"\"\n",
        "    unique_terms = set()  # Use a set to store unique terms\n",
        "    with open(csv_file, mode='r', encoding='utf-8') as file:\n",
        "        reader = csv.reader(file)\n",
        "        next(reader)  # Skip the header row\n",
        "        for row in reader:\n",
        "            if row:  # Ensure the row is not empty\n",
        "                unique_terms.add(row[0].strip())  # Add the term to the set\n",
        "    return unique_terms\n",
        "\n",
        "# Main function to count occurrences of terms and save results\n",
        "def search_and_count_terms(csv_file, text_file, output_file):\n",
        "    \"\"\"\n",
        "    Reads terms from a CSV file, counts their occurrences in a text file, \n",
        "    and writes the results to an output file.\n",
        "\n",
        "    Args:\n",
        "        csv_file (str): Path to the CSV file containing terms.\n",
        "        text_file (str): Path to the text file to search in.\n",
        "        output_file (str): Path to the output file to save results.\n",
        "    \"\"\"\n",
        "    unique_terms = extract_unique_terms(csv_file)\n",
        "    with open(output_file, 'w', encoding='utf-8') as output:\n",
        "        for term in unique_terms:\n",
        "            occurrences = count_term_occurrences(term, text_file)\n",
        "            output.write(f\"{term}: appears {occurrences} time(s).\\n\")\n",
        "\n",
        "# File paths\n",
        "csv_file_path = \"spanish/[Physics ~ Electro-magnetism]_and_[Technological Sciences ~ Electrical technology and engineering].csv\"\n",
        "text_file_path = \"spanish/depth_1/specific_wikipedia_files/eswiki_electromagnetism_depth1.txt\"  # Replace with the path to your text file\n",
        "output_file_path = \"spanish/depth_1/terms_counter/terms_number_electromagnetism.txt\"  # File to save the results\n",
        "\n",
        "# Execute the main function\n",
        "search_and_count_terms(csv_file_path, text_file_path, output_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVwMXnSGXA5P"
      },
      "source": [
        "## Second code with better handling of compound terms. V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUnECSNCXAWw",
        "outputId": "4e10c19b-ba64-4631-a6f8-ba0946c50a78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ secci√≥n: 302 time(s) in the corpus\n",
            "‚úÖ rango: 316 time(s) in the corpus\n",
            "‚úÖ proceso: 2077 time(s) in the corpus\n",
            "‚úÖ movilidad: 111 time(s) in the corpus\n",
            "‚úÖ efecto: 434 time(s) in the corpus\n",
            "‚úÖ evoluci√≥n: 153 time(s) in the corpus\n",
            "‚úÖ sucesi√≥n: 103 time(s) in the corpus\n",
            "‚úÖ medio: 1219 time(s) in the corpus\n",
            "‚úÖ muestra: 245 time(s) in the corpus\n",
            "‚úÖ plataforma continental: 1 time(s) in the corpus\n",
            "‚úÖ plan: 774 time(s) in the corpus\n",
            "‚úÖ base: 1050 time(s) in the corpus\n",
            "‚úÖ suspensi√≥n: 188 time(s) in the corpus\n",
            "‚úÖ acci√≥n: 1032 time(s) in the corpus\n",
            "‚úÖ afinidad: 12 time(s) in the corpus\n",
            "‚úÖ limitaci√≥n: 99 time(s) in the corpus\n",
            "‚úÖ medida preventiva: 5 time(s) in the corpus\n",
            "‚úÖ viabilidad: 23 time(s) in the corpus\n",
            "‚úÖ programa: 986 time(s) in the corpus\n",
            "‚úÖ medida: 831 time(s) in the corpus\n",
            "‚úÖ procesar: 41 time(s) in the corpus\n",
            "‚úÖ proyecto: 1606 time(s) in the corpus\n",
            "‚úÖ disolver: 116 time(s) in the corpus\n",
            "‚úÖ prueba: 280 time(s) in the corpus\n",
            "‚úÖ laguna: 11 time(s) in the corpus\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import re\n",
        "\n",
        "# Function to count exact occurrences of a term in a text file\n",
        "def count_exact_occurrences_v2(term, text_file):\n",
        "    \"\"\"\n",
        "    Counts the exact occurrences of a term in a text file.\n",
        "\n",
        "    Args:\n",
        "        term (str): The term to search for.\n",
        "        text_file (str): Path to the text file.\n",
        "\n",
        "    Returns:\n",
        "        int: The number of occurrences of the term in the text file.\n",
        "    \"\"\"\n",
        "    pattern = rf\"\\b{re.escape(term)}\\b\"  # \\b ensures exact word matching\n",
        "    count = 0\n",
        "\n",
        "    with open(text_file, 'r', encoding='utf-8') as file:\n",
        "        for line in file:  # Process line by line\n",
        "            count += len(re.findall(pattern, line, re.IGNORECASE))  # Count occurrences in the line\n",
        "\n",
        "    return count\n",
        "\n",
        "# Main function to count occurrences of terms and display results\n",
        "def search_terms_and_count_v2(csv_file, text_file, output_file):\n",
        "    \"\"\"\n",
        "    Reads terms from a CSV file, counts their occurrences in a text file, \n",
        "    and writes the results to an output file.\n",
        "\n",
        "    Args:\n",
        "        csv_file (str): Path to the CSV file containing terms.\n",
        "        text_file (str): Path to the text file to search in.\n",
        "        output_file (str): Path to the output file to save results.\n",
        "    \"\"\"\n",
        "    unique_terms = extract_unique_terms(csv_file)\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as output:\n",
        "        for term in unique_terms:\n",
        "            occurrences = count_exact_occurrences_v2(term, text_file)\n",
        "            output.write(f\"{term}: appears {occurrences} time(s).\\n\")\n",
        "            print(f\"‚úÖ {term}: {occurrences} time(s) in the corpus\" if occurrences > 0 else f\"‚ùå {term}: Not found in the corpus\")\n",
        "\n",
        "# üìå File paths\n",
        "csv_file_path_law = \"spanish/[Juridical Sciences & Law ~ Constitutional law]_and_[Technological Sciences ~ Environmental technology and engineering].csv\"\n",
        "text_file_path_law = \"spanish/depth_1/specific_wikipedia_files/eswiki_law_depth1.txt\"\n",
        "output_file_path_law = \"spanish/depth_1/terms_counter/terms_number_law_v2.txt\"\n",
        "\n",
        "# Execute\n",
        "search_terms_and_count_v2(csv_file_path_law, text_file_path_law, output_file_path_law)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPkmF6iESB-h"
      },
      "source": [
        "## Proposal for faster context extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXpXPXDlSE6_",
        "outputId": "e7f9d94c-8695-4678-f574-dea3c987b4b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ indicador biol√≥gico: 1 occurrences in the corpus\n",
            "‚úÖ cierre: 519 occurrences in the corpus\n",
            "‚úÖ cloruro: 601 occurrences in the corpus\n",
            "‚úÖ flujo: 1529 occurrences in the corpus\n",
            "‚úÖ dosificaci√≥n: 56 occurrences in the corpus\n",
            "‚úÖ brillo: 139 occurrences in the corpus\n",
            "‚úÖ enlace: 483 occurrences in the corpus\n",
            "‚úÖ medio: 7890 occurrences in the corpus\n",
            "‚úÖ producci√≥n: 8005 occurrences in the corpus\n",
            "‚úÖ sonda: 237 occurrences in the corpus\n",
            "‚úÖ material filtrante: 5 occurrences in the corpus\n",
            "‚úÖ componente: 754 occurrences in the corpus\n",
            "‚úÖ rayo: 175 occurrences in the corpus\n",
            "‚úÖ eficacia: 777 occurrences in the corpus\n",
            "‚úÖ polvo: 818 occurrences in the corpus\n",
            "‚úÖ substrato: 27 occurrences in the corpus\n",
            "‚úÖ determinaci√≥n: 303 occurrences in the corpus\n",
            "‚úÖ temperatura ambiente: 175 occurrences in the corpus\n",
            "‚úÖ productor: 467 occurrences in the corpus\n",
            "‚úÖ valoraci√≥n: 231 occurrences in the corpus\n",
            "‚úÖ ensayo: 513 occurrences in the corpus\n",
            "‚úÖ colorante: 70 occurrences in the corpus\n",
            "‚úÖ normativa: 407 occurrences in the corpus\n",
            "‚úÖ inspecci√≥n: 289 occurrences in the corpus\n",
            "‚úÖ n√∫cleo: 940 occurrences in the corpus\n",
            "‚úÖ ruptura: 283 occurrences in the corpus\n",
            "‚úÖ unidad de volumen: 15 occurrences in the corpus\n",
            "‚úÖ vapor: 1222 occurrences in the corpus\n",
            "‚úÖ √°cido clorh√≠drico: 59 occurrences in the corpus\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import re\n",
        "\n",
        "# Function to find exact matches and extract their context with the term highlighted\n",
        "def extract_context(term, text_file, context_chars=200):\n",
        "    \"\"\"\n",
        "    Extracts the context of exact matches of a term in a text file, highlighting the term.\n",
        "\n",
        "    Args:\n",
        "        term (str): The term to search for.\n",
        "        text_file (str): Path to the text file.\n",
        "        context_chars (int): Number of characters to include before and after the match.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of contexts where the term appears, with the term highlighted.\n",
        "    \"\"\"\n",
        "    pattern = rf\"\\b{re.escape(term)}\\b\"  # Exact word match\n",
        "    contexts = []\n",
        "\n",
        "    with open(text_file, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()  # Load the entire text\n",
        "\n",
        "    for match in re.finditer(pattern, content, re.IGNORECASE):\n",
        "        start = max(0, match.start() - context_chars)\n",
        "        end = min(len(content), match.end() + context_chars)\n",
        "        context = content[start:end].replace(\"\\n\", \" \")  # Remove line breaks\n",
        "\n",
        "        # Highlight the term with **asterisks**\n",
        "        highlighted_term = f\"**{content[match.start():match.end()]}**\"\n",
        "        highlighted_context = context[: match.start() - start] + highlighted_term + context[match.end() - start :]\n",
        "\n",
        "        contexts.append(highlighted_context.strip())\n",
        "\n",
        "    return contexts  # List of found contexts\n",
        "\n",
        "# Main function to search for terms and save their context\n",
        "def search_terms_and_save(csv_file, text_file, output_txt, output_csv):\n",
        "    \"\"\"\n",
        "    Reads terms from a CSV file, searches for their occurrences in a text file, \n",
        "    and saves the context of each occurrence to a TXT and CSV file.\n",
        "\n",
        "    Args:\n",
        "        csv_file (str): Path to the CSV file containing terms.\n",
        "        text_file (str): Path to the text file to search in.\n",
        "        output_txt (str): Path to the output TXT file to save contexts.\n",
        "        output_csv (str): Path to the output CSV file to save contexts.\n",
        "    \"\"\"\n",
        "    unique_terms = extract_unique_terms(csv_file)\n",
        "\n",
        "    with open(output_txt, 'w', encoding='utf-8') as txt_out, open(output_csv, 'w', encoding='utf-8', newline='') as csv_out:\n",
        "        csv_writer = csv.writer(csv_out)\n",
        "        csv_writer.writerow([\"Term\", \"Context\", \"Occurrences\"])  # CSV headers\n",
        "\n",
        "        for term in unique_terms:\n",
        "            contexts = extract_context(term, text_file)\n",
        "            occurrences = len(contexts)\n",
        "\n",
        "            # Save to TXT\n",
        "            txt_out.write(f\"\\n=== {term.upper()} ({occurrences} occurrences) ===\\n\")\n",
        "            if occurrences > 0:\n",
        "                for i, context in enumerate(contexts, 1):\n",
        "                    txt_out.write(f\"{i}. ...{context}...\\n\")\n",
        "            else:\n",
        "                txt_out.write(\"Not found in the corpus.\\n\")\n",
        "\n",
        "            # Save to CSV\n",
        "            for context in contexts:\n",
        "                csv_writer.writerow([term, context, occurrences])\n",
        "\n",
        "            # Print message to console\n",
        "            print(f\"‚úÖ {term}: {occurrences} occurrences in the corpus\" if occurrences > 0 else f\"‚ùå {term}: Not found in the corpus\")\n",
        "\n",
        "\n",
        "# üìå File paths\n",
        "csv_file_path_environmental = \"spanish/[Medical Sciences ~ Pharmaceutical Technology]_and_[Technological Sciences ~ Environmental technology and engineering].csv\"\n",
        "text_file_path_environmental = \"spanish/depth_4/specific_wikipedia_files/eswiki_environmental.txt\"\n",
        "output_txt_path_environmental = \"spanish/depth_4/contexts/context_terms_environmental.txt\"\n",
        "output_csv_path_environmental = \"spanish/depth_4/contexts/context_terms_environmental.csv\"\n",
        "\n",
        "# Execute\n",
        "search_terms_and_save(csv_file_path_environmental, text_file_path_environmental, output_txt_path_environmental, output_csv_path_environmental)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
